# TITLE

## @Decentralized Federated Learning: Balancing Communication and Computing Costs

这篇论文感觉没什么创新点(只是在本地节点完成多次本地更新后，再共识更新)，写得不怎么好

## @Decentralised federated learning with adaptive partial gradient aggregation

### CAAI 2020
提出的算法：FedPGA (Partial Gradient Exchange and Adaptive SGD)

创新点：部分梯度（降低通信开销） 仿Adam梯度下降方式（提升精度）

具体的做法是把梯度分成S小段，参与方向随机的S个其他参与方pull相应段的梯度，再把它们和本地的训练梯度聚合，形成混合梯度，利用这个混合梯度，用仿adam的方式进行模型更新。

### 具体细节

* 聚合：
  * 传的是什么：$\tau$-difference gradient -- 经过 $\tau$ 次本地梯度下降后的梯度与初始梯度的差值 ![img](https://img2023.cnblogs.com/blog/2145900/202307/2145900-20230707155706864-9739384.png)
  * 采用的聚合方式： 平均
   ![img](https://img2023.cnblogs.com/blog/2145900/202307/2145900-20230707155803752-385865496.png)
* 梯度下降方式：仿Adam
  * ![img](https://img2023.cnblogs.com/blog/2145900/202307/2145900-20230707161007532-596115106.png)
  * ![img](https://img2023.cnblogs.com/blog/2145900/202307/2145900-20230707161026716-1346468083.png)
* 网络拓扑: 全连接

## @Decentralized Federated Learning with Data Feature Transmission and Neighbor Selection

### 2022 IEEE 28th International Conference on Parallel and Distributed Systems (ICPADS)

* 提出的算法：DFL-DF
* 创新点：
  * 传数据特征和数据特征提取器 而不是传模型（**减少通讯开销**）
  * 可变通讯拓扑（**可缓解Non-IID问题**，尽量在**数据分布不一致**的节点之间交换信息，使模型学习到各个节点的数据）

---

* 做法是先每个节点先本地训练一定的轮次，然后将数据特征和feature extractor参数与邻居节点进行交换。
  * 通过平均聚合feature extractor的参数形成本地feature extractor（上游特征提取器），
  * 通过从其他节点处接受的数据特征输入到本地classifier训练出新的classifier（下游分类器）
* 每个节点会将数据特征发给协调段（起辅助计算作用），服务器计算每个节点数据特征之间的相似度来决定下一轮的通讯拓扑：
  
  * ``` python
    # 下面这一段看不懂没有关系，因为我自己也不能保证今后能不能看懂哈哈哈
    if (N_{ij}^t == 1 即在第t轮迭代中两个节点之间有边的连接) {
      服务器将计算两个节点之间的余弦相似度，根据余弦相似度与该边的历史选择次数来决定该边下一轮t+1轮被选中的概率。
      余弦相似度越低，或者该边历史被选中的次数越多，那么该边被选中的概率就越高。
    } elif (N_{ij}^t == 0) {
      服务器将按照上一轮的选择该边的概率在这一轮选择该边。
    }
    ```

  * 总之，通信拓扑每一轮是动态变化的，主要思想是选取与自身数据分布不一致的节点作为邻居节点，来交换数据特征和feature extractor。
  * 公式如下：
  * ![img](https://img2023.cnblogs.com/blog/2145900/202307/2145900-20230709214740871-2143954150.png)
  * 其中 $p_{ij}^{t+1}$ 是节点 i 和节点 j 在下一轮之间有边连接的概率, $m_{ij}^t$是边 ij 的历史选中次数，$p_{ij}'$ 是根据节点i和节点j数据特征的余弦相似度计算出来的概率，相似度越低，$p_{ij}'$ 越高。

### 具体对比细节

* 聚合：
  * 传的是 extractor，
  * 采用平均聚合
* 梯度下降方式：SGD
* 网络拓扑：全连接，理论上所有邻居节点之间都有进行通讯的机会，但是有邻居选择机制。